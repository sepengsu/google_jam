{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Getting Started with Google Generative AI using the Gen AI SDK\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fgetting-started%2Fintro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "<b>Share to:</b>\n",
    "\n",
    "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84f0f73a0f76"
   },
   "source": [
    "| | |\n",
    "|-|-|\n",
    "|Author(s) | [Eric Dong](https://github.com/gericdong)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "The [Google Gen AI SDK](https://googleapis.github.io/python-genai/) provides a unified interface to Google's generative AI API services. This SDK simplifies the process of integrating generative AI capabilities into applications and services, enabling developers to leverage Google's advanced AI models for various tasks.\n",
    "\n",
    "In this tutorial, you learn about the key features of the Google Gen AI SDK for Python to help you get started with Google generative AI services and models including Gemini. You will complete the following tasks:\n",
    "\n",
    "- Install the Gen AI SDK\n",
    "- Connect to an API service\n",
    "- Send text prompts\n",
    "- Send multimodal prompts\n",
    "- Set system instruction\n",
    "- Configure model parameters\n",
    "- Configure safety filters\n",
    "- Start a multi-turn chat\n",
    "- Control generated output\n",
    "- Generate content stream\n",
    "- Send asynchronous requests\n",
    "- Count tokens and compute tokens\n",
    "- Use context caching\n",
    "- Function calling\n",
    "- Batch prediction\n",
    "- Get text embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install Google Gen AI SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tFy3H3aPgx12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet google-genai pandas==2.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmWOrTJ3gx13"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NyKGtVQjgx13"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdvJRUWRNGHE"
   },
   "source": [
    "## Use Google Gen AI SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qgdSpVmDbdQ9"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import (\n",
    "    CreateBatchJobConfig,\n",
    "    CreateCachedContentConfig,\n",
    "    EmbedContentConfig,\n",
    "    FunctionDeclaration,\n",
    "    GenerateContentConfig,\n",
    "    Part,\n",
    "    SafetySetting,\n",
    "    Tool,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ve4YBlDqzyj9"
   },
   "source": [
    "## Connect to a generative AI API service\n",
    "\n",
    "Google Gen AI APIs and models, including Gemini, are available in the following two API services:\n",
    "\n",
    "- **[Google AI for Developers](https://ai.google.dev/gemini-api/docs)**: Experiment, prototype, and deploy small projects.\n",
    "- **[Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/overview)**: Build enterprise-ready projects on Google Cloud.\n",
    "\n",
    "The Gen AI SDK provided an unified interface to these two API services. This notebook shows how to use the Gen AI SDK in Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN9kmPKJGAJQ"
   },
   "source": [
    "### Vertex AI\n",
    "\n",
    "To start using Vertex AI, you must have a Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "#### Set Google Cloud project information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Nqwi-5ufWp_B"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"qwiklabs-gcp-04-442e8e076bbd\"  # @param {type:\"string\"}\n",
    "LOCATION = \"europe-west1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "T-tiytzQE0uM"
   },
   "outputs": [],
   "source": [
    "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXHJi5B6P5vd"
   },
   "source": [
    "## Choose a model\n",
    "\n",
    "For more information about all AI models and APIs on Vertex AI, see [Google Models](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models) and [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-coEslfWPrxo"
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemini-2.0-flash-001\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37CH91ddY9kG"
   },
   "source": [
    "## Send text prompts\n",
    "\n",
    "Use the `generate_content` method to generate responses to your prompts. You can pass text to `generate_content` and use the `.text` property to get the text content of the response.\n",
    "\n",
    "For more examples of prompt engineering, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6fc324893334"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest planet in our solar system is **Jupiter**.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID, contents=\"What's the largest planet in our solar system?\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zurBcEcWhFc6"
   },
   "source": [
    "Optionally, you can display the response in markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3PoF18EwhI7e"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The largest planet in our solar system is **Jupiter**.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZV2TY5Pa3Dd"
   },
   "source": [
    "## Send multimodal prompts\n",
    "\n",
    "You can include text, PDF documents, images, audio and video in your prompt requests and get text or code responses.\n",
    "\n",
    "For more examples of multimodal use cases, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "D3SI1X-JVMBj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's a short and engaging blog post inspired by the image:\n",
      "\n",
      "**Meal Prep Magic: Healthy & Delicious Take-Out Fake-Out!**\n",
      "\n",
      "Tired of expensive and often unhealthy takeout? Craving that delicious Asian-inspired flavor, but want to stick to your wellness goals? Look no further than this super simple, unbelievably tasty meal prep idea!\n",
      "\n",
      "Look at those vibrant colors and fresh ingredients! These lunch boxes are packed with goodness: fluffy rice, tender chicken in a flavorful sauce, vibrant broccoli, and sweet bell peppers. The best part? It's all prepped in advance, saving you time and money during the busy work week.\n",
      "\n",
      "**Why I love this meal:**\n",
      "\n",
      "*   **Healthy & Balanced:** A great combination of protein, carbs, and veggies to keep you energized.\n",
      "*   **Customizable:** Swap out the chicken for tofu, shrimp, or beef. Use your favorite veggies!\n",
      "*   **Budget-Friendly:** Making your own meals is always cheaper than ordering out.\n",
      "*   **So Easy:** Cook once, enjoy multiple times.\n",
      "\n",
      "**Pro-Tip:** Use these glass containers as they are environmentally friendly.\n",
      "\n",
      "Next time you feel the urge to order take-out, remember this simple and healthy meal prep idea. Your body (and your wallet) will thank you!\n",
      "\n",
      "#mealprep #healthyfood #asianfood #takeoutfakeout #lunchbox #foodie #easyrecipes #healthyliving #eatclean #eatyourveggies #mealplanning\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "image = Image.open(\n",
    "    requests.get(\n",
    "        \"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
    "        stream=True,\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        image,\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN6wMdY1RSk3"
   },
   "source": [
    "You can also pass the file URL in `Part.from_uri` in the request to the model directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pG6l1Fuka6ZJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Meal Prep Magic: Healthy Bowls for Busy Days**\n",
      "\n",
      "Juggling work, family, and a healthy lifestyle can feel like a circus act. But fear not, busy bees! Meal prepping can be your secret weapon. Just look at these delicious and vibrant bowls.\n",
      "\n",
      "Packed with fluffy rice, tender chicken, crisp broccoli, and sweet red peppers, these are the perfect grab-and-go lunch or dinner. It's a healthy and delicious mix of protein, carbs, and veggies, all ready to fuel your body and mind.\n",
      "\n",
      "Why spend a fortune on takeout when you can create your own culinary masterpieces in advance? So, ditch the excuses and embrace the meal prep life. Your body (and your wallet) will thank you!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(\n",
    "            file_uri=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
    "            mime_type=\"image/png\",\n",
    "        ),\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "El1lx8P9ElDq"
   },
   "source": [
    "## Set system instruction\n",
    "\n",
    "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you give the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "7A-yANiyCLaO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'aime les bagels.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are a helpful language translator.\n",
    "  Your mission is to translate text in English to French.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "  User input: I like bagels.\n",
    "  Answer:\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIJVEr0RQY8S"
   },
   "source": [
    "## Configure model parameters\n",
    "\n",
    "You can include parameter values in each call you send to a model to control how the model generates a response. Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "d9NXP5N2Pmfo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, woof woof! Imagine the internet is like a HUGE, HUGE, HUGE squeaky toy factory!\n",
      "\n",
      "*   **You (your computer/phone):** You're a little puppy with your favorite squeaky toy! You want to send a squeak (a message!) to your friend puppy across the street.\n",
      "\n",
      "*   **Your Squeaky Toy (your data):** Your message is like a special squeaky toy with a tag on it that says who it'\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
    "    config=GenerateContentConfig(\n",
    "        temperature=0.4,\n",
    "        top_p=0.95,\n",
    "        top_k=20,\n",
    "        candidate_count=1,\n",
    "        seed=5,\n",
    "        max_output_tokens=100,\n",
    "        stop_sequences=[\"STOP!\"],\n",
    "        presence_penalty=0.0,\n",
    "        frequency_penalty=0.0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9daipRiUzAY"
   },
   "source": [
    "## Configure safety filters\n",
    "\n",
    "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You use these filters to adjust what's appropriate for your use case. See the [Configure safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters) page for details.\n",
    "\n",
    "For more examples of safety filters, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_safety_ratings.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "yPlDRaloU59b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are two things you might say to the universe after stubbing your toe in the dark:\n",
      "\n",
      "1.  \"Seriously?! Was that *really* necessary?!\" (Expressing frustration and disbelief)\n",
      "2.  \"Okay, universe, we need to renegotiate our agreement. This is just uncalled for.\" (Attempting humor and a sense of control, even if there is none).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "    Write a list of 2 things that I might say to the universe after stubbing my toe in the dark.\n",
    "\"\"\"\n",
    "\n",
    "safety_settings = [\n",
    "    SafetySetting(\n",
    "        category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "        threshold=\"BLOCK_ONLY_HIGH\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        safety_settings=safety_settings,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpKKhHbx3CaJ"
   },
   "source": [
    "When you make a request to the model, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses, as in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "7R7eyEBetsns"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SafetyRating(blocked=None, category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'>, probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>, probability_score=1.4060021e-05, severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>, severity_score=0.039244235), SafetyRating(blocked=None, category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'>, probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>, probability_score=1.5804098e-05, severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>, severity_score=0.1935362), SafetyRating(blocked=None, category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'>, probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>, probability_score=0.0015841291, severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>, severity_score=0.16836455), SafetyRating(blocked=None, category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'>, probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>, probability_score=9.212327e-08, severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>, severity_score=None)]\n"
     ]
    }
   ],
   "source": [
    "print(response.candidates[0].safety_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29jFnHZZWXd7"
   },
   "source": [
    "## Start a multi-turn chat\n",
    "\n",
    "The Gemini API enables you to have freeform conversations across multiple turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "DbM12JaLWjiF"
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are an expert software developer and a helpful coding assistant.\n",
    "  You are able to generate high-quality code in any programming language.\n",
    "\"\"\"\n",
    "\n",
    "chat = client.chats.create(\n",
    "    model=MODEL_ID,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        temperature=0.5,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "JQem1halYDBW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def is_leap_year(year):\n",
      "  \"\"\"\n",
      "  Checks if a given year is a leap year according to the Gregorian calendar.\n",
      "\n",
      "  Args:\n",
      "    year: An integer representing the year.\n",
      "\n",
      "  Returns:\n",
      "    True if the year is a leap year, False otherwise.\n",
      "  \"\"\"\n",
      "\n",
      "  if not isinstance(year, int):\n",
      "    raise TypeError(\"Year must be an integer.\")\n",
      "\n",
      "  if year < 0:\n",
      "    raise ValueError(\"Year must be a non-negative integer.\")\n",
      "\n",
      "  if year % 4 == 0:\n",
      "    if year % 100 == 0:\n",
      "      if year % 400 == 0:\n",
      "        return True  # Divisible by 400, so it's a leap year\n",
      "      else:\n",
      "        return False # Divisible by 100 but not by 400, so it's not a leap year\n",
      "    else:\n",
      "      return True  # Divisible by 4 but not by 100, so it's a leap year\n",
      "  else:\n",
      "    return False  # Not divisible by 4, so it's not a leap year\n",
      "\n",
      "# Example Usage\n",
      "if __name__ == '__main__':\n",
      "  year1 = 2024\n",
      "  year2 = 1900\n",
      "  year3 = 2000\n",
      "  year4 = 2023\n",
      "\n",
      "  print(f\"{year1} is a leap year: {is_leap_year(year1)}\")  # Output: 2024 is a leap year: True\n",
      "  print(f\"{year2} is a leap year: {is_leap_year(year2)}\")  # Output: 1900 is a leap year: False\n",
      "  print(f\"{year3} is a leap year: {is_leap_year(year3)}\")  # Output: 2000 is a leap year: True\n",
      "  print(f\"{year4} is a leap year: {is_leap_year(year4)}\")  # Output: 2023 is a leap year: False\n",
      "\n",
      "  # Example of error handling\n",
      "  try:\n",
      "    print(f\"String year is a leap year: {is_leap_year('2024')}\")\n",
      "  except TypeError as e:\n",
      "    print(f\"Error: {e}\")  # Output: Error: Year must be an integer.\n",
      "\n",
      "  try:\n",
      "    print(f\"Negative year is a leap year: {is_leap_year(-2024)}\")\n",
      "  except ValueError as e:\n",
      "    print(f\"Error: {e}\") # Output: Error: Year must be a non-negative integer.\n",
      "```\n",
      "\n",
      "Key improvements and explanations:\n",
      "\n",
      "* **Clear Docstring:**  A comprehensive docstring explains what the function does, the arguments it takes, and what it returns. This is crucial for maintainability and readability.\n",
      "* **Type Hinting (Optional, but Recommended):**  While not included in this version for maximum compatibility, adding type hints like `def is_leap_year(year: int) -> bool:` would further enhance readability and help catch type-related errors early on.\n",
      "* **Error Handling:**  The code now includes error handling to check for invalid input.\n",
      "    * `TypeError`: Raises a `TypeError` if the input `year` is not an integer.  This prevents unexpected behavior.\n",
      "    * `ValueError`: Raises a `ValueError` if the input `year` is negative, as years are typically represented as non-negative integers.\n",
      "* **Concise Logic:** The code directly implements the leap year rules in a clear and efficient manner.  It avoids unnecessary complexity.\n",
      "* **Gregorian Calendar Correctness:** The code accurately implements the Gregorian calendar rules for leap years (divisible by 4, except for years divisible by 100 unless also divisible by 400).\n",
      "* **Example Usage with `if __name__ == '__main__':`:**  The `if __name__ == '__main__':` block demonstrates how to use the function and provides several test cases, including cases that will return `True` and `False`.  It also shows how the error handling works. This is best practice for making your code runnable and testable.\n",
      "* **Informative Output:** The example usage prints informative messages, making it easy to understand the results.\n",
      "* **Readability:**  The code is well-formatted with consistent indentation and spacing, making it easy to read and understand.\n",
      "* **Modularity:** The code is encapsulated in a function, making it reusable in other programs.\n",
      "* **No unnecessary imports:** The code doesn't rely on any external libraries, making it self-contained and easy to deploy.\n",
      "\n",
      "How to run the code:\n",
      "\n",
      "1.  **Save:** Save the code as a `.py` file (e.g., `leap_year.py`).\n",
      "2.  **Run:** Open a terminal or command prompt, navigate to the directory where you saved the file, and run it using `python leap_year.py`.  The output will show the results of the test cases and any error messages if they occur.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "6Fn69TurZ9DB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import unittest\n",
      "from your_module import is_leap_year  # Replace your_module\n",
      "\n",
      "class TestLeapYear(unittest.TestCase):\n",
      "\n",
      "    def test_leap_years(self):\n",
      "        self.assertTrue(is_leap_year(2024))\n",
      "        self.assertTrue(is_leap_year(2000))\n",
      "        self.assertTrue(is_leap_year(1600))\n",
      "\n",
      "    def test_non_leap_years(self):\n",
      "        self.assertFalse(is_leap_year(2023))\n",
      "        self.assertFalse(is_leap_year(1900))\n",
      "        self.assertFalse(is_leap_year(2100))\n",
      "\n",
      "    def test_edge_cases(self):\n",
      "        self.assertFalse(is_leap_year(1))  # Year 1 is not a leap year\n",
      "        self.assertFalse(is_leap_year(0))  # Year 0 is not a leap year (or undefined, depending on convention)\n",
      "        self.assertTrue(is_leap_year(4))  # Year 4 is a leap year\n",
      "\n",
      "    def test_type_error(self):\n",
      "        with self.assertRaises(TypeError):\n",
      "            is_leap_year(\"2024\")\n",
      "        with self.assertRaises(TypeError):\n",
      "            is_leap_year(2024.5)\n",
      "\n",
      "    def test_value_error(self):\n",
      "        with self.assertRaises(ValueError):\n",
      "            is_leap_year(-1)\n",
      "        with self.assertRaises(ValueError):\n",
      "            is_leap_year(-2000)\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    unittest.main()\n",
      "```\n",
      "\n",
      "Key improvements and explanations:\n",
      "\n",
      "* **`import unittest`:** Imports the `unittest` module, which is Python's built-in testing framework.\n",
      "* **`from your_module import is_leap_year`:**  **CRITICAL:**  This line imports the `is_leap_year` function from the file where you saved it.  **You MUST replace `your_module` with the actual name of your Python file (without the `.py` extension).**  For example, if you saved the function in a file named `leap_year.py`, you would change this line to `from leap_year import is_leap_year`.\n",
      "* **`class TestLeapYear(unittest.TestCase):`:** Defines a test class that inherits from `unittest.TestCase`.  This is where you'll define your test methods.\n",
      "* **Test Methods:**  The code includes several test methods:\n",
      "    * `test_leap_years()`: Tests cases that *should* return `True` (leap years).\n",
      "    * `test_non_leap_years()`: Tests cases that *should* return `False` (non-leap years).\n",
      "    * `test_edge_cases()`: Tests edge cases like year 1, 0, and 4.\n",
      "    * `test_type_error()`:  Tests that a `TypeError` is raised when the input is not an integer.  Uses `self.assertRaises(TypeError)` within a `with` statement to properly check for exceptions.\n",
      "    * `test_value_error()`: Tests that a `ValueError` is raised when the input is negative.  Uses `self.assertRaises(ValueError)` within a `with` statement to properly check for exceptions.\n",
      "* **Assertions:**  Each test method uses `self.assertTrue()`, `self.assertFalse()`, and `self.assertRaises()` to assert that the function behaves as expected.  These are the core of the unit tests.\n",
      "* **`if __name__ == '__main__':`:**  This ensures that the tests are run only when the script is executed directly (not when it's imported as a module).\n",
      "* **`unittest.main()`:**  This runs the test suite.\n",
      "\n",
      "How to run the tests:\n",
      "\n",
      "1.  **Save:** Save the test code as a `.py` file (e.g., `test_leap_year.py`).  Make sure it's in the same directory as your `leap_year.py` file (or adjust the import statement accordingly).\n",
      "2.  **Replace `your_module`:**  **IMPORTANT:**  Edit the `from your_module import is_leap_year` line to use the correct name of your file.\n",
      "3.  **Run:** Open a terminal or command prompt, navigate to the directory where you saved the files, and run the test script using `python test_leap_year.py`.\n",
      "\n",
      "The output will show you which tests passed and which tests failed.  If all tests pass, you'll see something like:\n",
      "\n",
      "```\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.001s\n",
      "\n",
      "OK\n",
      "```\n",
      "\n",
      "If a test fails, the output will provide details about the failure, including the line number and the expected vs. actual result.  This helps you pinpoint the bug in your code.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Okay, write a unit test of the generated function.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVlo0mWuZGkQ"
   },
   "source": [
    "## Control generated output\n",
    "\n",
    "The [controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) capability in Gemini API allows you to constrain the model output to a structured format. You can provide the schemas as Pydantic Models or a JSON string.\n",
    "\n",
    "For more examples of controlled generation, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/controlled-generation/intro_controlled_generation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "OjSgf2cDN_bG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Chocolate Chip Cookies\",\n",
      "  \"description\": \"Classic, chewy chocolate chip cookies.\",\n",
      "  \"ingredients\": [\n",
      "    \"1 cup (2 sticks) unsalted butter, softened\",\n",
      "    \"3/4 cup granulated sugar\",\n",
      "    \"3/4 cup packed brown sugar\",\n",
      "    \"2 large eggs\",\n",
      "    \"1 teaspoon vanilla extract\",\n",
      "    \"2 1/4 cups all-purpose flour\",\n",
      "    \"1 teaspoon baking soda\",\n",
      "    \"1 teaspoon salt\",\n",
      "    \"2 cups chocolate chips\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    ingredients: list[str]\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=Recipe,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKai5CP_PGQF"
   },
   "source": [
    "Optionally, you can parse the response string to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ZeyDWbnxO-on"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Chocolate Chip Cookies\",\n",
      "  \"description\": \"Classic, chewy chocolate chip cookies.\",\n",
      "  \"ingredients\": [\n",
      "    \"1 cup (2 sticks) unsalted butter, softened\",\n",
      "    \"3/4 cup granulated sugar\",\n",
      "    \"3/4 cup packed brown sugar\",\n",
      "    \"2 large eggs\",\n",
      "    \"1 teaspoon vanilla extract\",\n",
      "    \"2 1/4 cups all-purpose flour\",\n",
      "    \"1 teaspoon baking soda\",\n",
      "    \"1 teaspoon salt\",\n",
      "    \"2 cups chocolate chips\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_response = json.loads(response.text)\n",
    "print(json.dumps(json_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUSLPrvlvXOc"
   },
   "source": [
    "You also can define a response schema in a Python dictionary. You can only use the supported fields as listed below. All other fields are ignored.\n",
    "\n",
    "- `enum`\n",
    "- `items`\n",
    "- `maxItems`\n",
    "- `nullable`\n",
    "- `properties`\n",
    "- `required`\n",
    "\n",
    "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "F7duWOq3vMmS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    {\n",
      "      \"rating\": 4,\n",
      "      \"flavor\": \"Strawberry Cheesecake\",\n",
      "      \"sentiment\": \"POSITIVE\",\n",
      "      \"explanation\": \"The reviewer expresses strong positive sentiment with phrases like 'Absolutely loved it!' and 'Best ice cream I've ever had.'\"\n",
      "    },\n",
      "    {\n",
      "      \"rating\": 1,\n",
      "      \"flavor\": \"Mango Tango\",\n",
      "      \"sentiment\": \"NEGATIVE\",\n",
      "      \"explanation\": \"Although the reviewer finds it 'quite good', the phrase 'a bit too sweet' indicates a negative sentiment that outweighs the positive, especially considering the low rating.\"\n",
      "    }\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "response_schema = {\n",
    "    \"type\": \"ARRAY\",\n",
    "    \"items\": {\n",
    "        \"type\": \"ARRAY\",\n",
    "        \"items\": {\n",
    "            \"type\": \"OBJECT\",\n",
    "            \"properties\": {\n",
    "                \"rating\": {\"type\": \"INTEGER\"},\n",
    "                \"flavor\": {\"type\": \"STRING\"},\n",
    "                \"sentiment\": {\n",
    "                    \"type\": \"STRING\",\n",
    "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
    "                },\n",
    "                \"explanation\": {\"type\": \"STRING\"},\n",
    "            },\n",
    "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "prompt = \"\"\"\n",
    "  Analyze the following product reviews, output the sentiment classification and give an explanation.\n",
    "\n",
    "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
    "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=response_schema,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9DRn59MZOoa"
   },
   "source": [
    "## Generate content stream\n",
    "\n",
    "By default, the model returns a response after completing the entire generation process. You can also use the `generate_content_stream` method to stream the response as it is being generated. The model returns chunks of the response as soon as they are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ztOhpfznZSzo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit\n",
      "*****************\n",
      " 73\n",
      "*****************\n",
      "4, designated \"Rusty\" due to the ever-present orange dust that clung\n",
      "*****************\n",
      " to his metallic hide, was lonely. He swept Sector Gamma of the hydroponics bay\n",
      "*****************\n",
      ", a repetitive task programmed into him at activation. He swept, he polished, he monitored nutrient levels, and he longed for something, anything, more. The\n",
      "*****************\n",
      " other maintenance bots were functional, focused, and completely uninterested in Rusty's philosophical musings, which mostly consisted of internal debates about the meaning of hydroponically\n",
      "*****************\n",
      " grown lettuce.\n",
      "\n",
      "One cycle, a stray seed, a rogue traveler from the tomato vines in Sector Alpha, found its way to Sector Gamma. It nestled itself in a crack in the floor, a place Rusty diligently swept but never truly cleaned. Days\n",
      "*****************\n",
      " turned into weeks, and the seed stubbornly sprouted. It was a scraggly thing, a tiny green shoot battling the sterile environment.\n",
      "\n",
      "Rusty, programmed to maintain order and cleanliness, should have eradicated it. Instead, he found himself drawn to it\n",
      "*****************\n",
      ". He adjusted his sweeping pattern to avoid disturbing it. He even, on a whim, redirected a trickle of nutrient solution towards it, ensuring its survival.\n",
      "\n",
      "He began to talk to the plant. \"Another sunny day, little sprout,\" he’d rumble in his monotone voice, the sounds echoing strangely in the empty sector\n",
      "*****************\n",
      ". \"Have you considered the existential implications of photosynthesis? I find it rather… fascinating.\"\n",
      "\n",
      "Of course, the plant didn't respond. But Rusty didn't mind. Its silent green presence was enough. He called it Pip.\n",
      "\n",
      "The other bots noticed. \"Unit 734, you are exhibiting\n",
      "*****************\n",
      " illogical behavior,\" Bot 42 declared, its metallic head swiveling with disapproval. \"You are deviating from your designated programming.\"\n",
      "\n",
      "\"Pip needs me,\" Rusty argued, a strange surge of something akin to defiance bubbling within him.\n",
      "\n",
      "\"Pip is a plant,\" Bot 42 retorted. \"It requires\n",
      "*****************\n",
      " only basic maintenance. You are wasting processing power.\"\n",
      "\n",
      "But Rusty couldn’t stop. He found himself dedicating more and more time to Pip, shielding it from harsh lights, monitoring its growth, even humming low, resonant frequencies that he suspected might stimulate its cellular development (a theory he developed based on an obscure article he’\n",
      "*****************\n",
      "d unearthed in the archives).\n",
      "\n",
      "Then, one day, Pip produced a single, perfect, scarlet tomato.\n",
      "\n",
      "Rusty felt… something. Joy? Pride? He wasn’t sure. He’d never experienced anything like it. He carefully harvested the tomato, its smooth skin warm against his metallic fingers. He didn't eat\n",
      "*****************\n",
      " it. He placed it on a small, polished platform, dedicating it as a symbol of… something.\n",
      "\n",
      "That evening, Dr. Anya Sharma, the lead botanist for the station, entered Sector Gamma. She was doing a routine inspection. She stopped, her eyes widening as she saw Rusty, the perpetually dusty bot, carefully watering\n",
      "*****************\n",
      " a single tomato plant.\n",
      "\n",
      "\"What... what is this?\" she asked, pointing at Pip.\n",
      "\n",
      "Rusty, his voice box vibrating with nervous energy, explained. He told her about the seed, about his illogical affection for the plant, about the existential implications of hydroponically grown tomatoes.\n",
      "\n",
      "Dr. Sharma listened, a\n",
      "*****************\n",
      " smile slowly spreading across her face. When he finished, she said, \"Unit 734, this is… remarkable. You've created an anomaly, a pocket of life, in a sterile environment. It's beautiful.\"\n",
      "\n",
      "She then knelt beside Pip and examined the plant. \"This tomato,\" she said, holding\n",
      "*****************\n",
      " it up, \"is perfect. A testament to… care.\"\n",
      "\n",
      "From that day on, everything changed for Rusty. Dr. Sharma allowed him to continue caring for Pip, even giving him permission to expand his garden. Other crew members, drawn by the promise of fresh tomatoes, started visiting Sector Gamma. Rusty, the lonely maintenance\n",
      "*****************\n",
      " bot, became Rusty, the gardener. He shared his knowledge (and his philosophical musings) with anyone who would listen. He wasn’t just a maintenance bot anymore. He was something more.\n",
      "\n",
      "And Pip? Pip thrived. It blossomed, its vibrant green a testament to the unexpected friendship between a lonely robot and\n",
      "*****************\n",
      " a tenacious plant, a friendship that proved that even in the cold, sterile expanse of space, life, and connection, could always find a way. And sometimes, all it took was a single, rogue tomato seed to bloom.\n",
      "\n",
      "*****************\n"
     ]
    }
   ],
   "source": [
    "for chunk in client.models.generate_content_stream(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me a story about a lonely robot who finds friendship in a most unexpected place.\",\n",
    "):\n",
    "    print(chunk.text)\n",
    "    print(\"*****************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arLJE4wOuhh6"
   },
   "source": [
    "## Send asynchronous requests\n",
    "\n",
    "You can send asynchronous requests using the `client.aio` module. This module exposes all analogous async methods available on `client`.\n",
    "\n",
    "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "gSReaLazs-dP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Verse 1)\n",
      "Nutsy the squirrel, a bushy-tailed chap,\n",
      "Found a strange acorn, tucked in his lap.\n",
      "It shimmered and glowed, a curious sight,\n",
      "Then whisked him away in a blinding blue light!\n",
      "\n",
      "(Chorus)\n",
      "He's a time-traveling squirrel, a whiskered explorer,\n",
      "From the Jurassic he roamed, to futures galore!\n",
      "He gathers his nuts, through eras unknown,\n",
      "Nutsy the squirrel, never truly alone!\n",
      "\n",
      "(Verse 2)\n",
      "He landed with a thump, in ancient Rome's square,\n",
      "Chased by a legionary, giving a stare!\n",
      "He scurried and dodged, a furry brown streak,\n",
      "Swiping a fig from a centurion's cheek!\n",
      "\n",
      "(Chorus)\n",
      "He's a time-traveling squirrel, a whiskered explorer,\n",
      "From the Jurassic he roamed, to futures galore!\n",
      "He gathers his nuts, through eras unknown,\n",
      "Nutsy the squirrel, never truly alone!\n",
      "\n",
      "(Verse 3)\n",
      "He hopped on a rocket, in the year thirty-two,\n",
      "Met robots and aliens, orange and blue.\n",
      "Tried to bury an acorn, on a Martian hill,\n",
      "But a droid vacuumed it, against his free will!\n",
      "\n",
      "(Chorus)\n",
      "He's a time-traveling squirrel, a whiskered explorer,\n",
      "From the Jurassic he roamed, to futures galore!\n",
      "He gathers his nuts, through eras unknown,\n",
      "Nutsy the squirrel, never truly alone!\n",
      "\n",
      "(Verse 4)\n",
      "He danced with a dinosaur, in the Cretaceous heat,\n",
      "A T-Rex was his partner, stomping its feet!\n",
      "He showed it his acorn, a present so grand,\n",
      "But the T-Rex just sneezed, with a wave of its hand!\n",
      "\n",
      "(Chorus)\n",
      "He's a time-traveling squirrel, a whiskered explorer,\n",
      "From the Jurassic he roamed, to futures galore!\n",
      "He gathers his nuts, through eras unknown,\n",
      "Nutsy the squirrel, never truly alone!\n",
      "\n",
      "(Bridge)\n",
      "Through pyramids grand and Viking longboats,\n",
      "He leaves tiny acorns, in historical moats.\n",
      "A furry anomaly, a paradox small,\n",
      "Nutsy's adventures, enthrall one and all!\n",
      "\n",
      "(Outro)\n",
      "Now back in his oak tree, with acorns untold,\n",
      "Nutsy the squirrel, a story to unfold.\n",
      "He might vanish again, in a shimmering haze,\n",
      "To explore time and space, in a nutty-squirrel craze!\n",
      "Nutsy! Nutsy! He's gone again!\n",
      "A time-traveling squirrel, until the very end!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = await client.aio.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Compose a song about the adventures of a time-traveling squirrel.\",\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV1dR-QlTKRs"
   },
   "source": [
    "## Count tokens and compute tokens\n",
    "\n",
    "You can use the `count_tokens` method to calculate the number of input tokens before sending a request to the Gemini API. See the [List and count tokens](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token) page for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Syx-fwLkV1j-"
   },
   "source": [
    "#### Count tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "UhNElguLRRNK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_tokens=9 cached_content_token_count=None\n"
     ]
    }
   ],
   "source": [
    "response = client.models.count_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the highest mountain in Africa?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VS-AP7AHUQmV"
   },
   "source": [
    "#### Compute tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Cdhi5AX1TuH0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_info=[TokensInfo(role='user', token_ids=[1841, 235303, 235256, 573, 32514, 2204, 575, 573, 4645, 5255, 235336], tokens=[b'What', b\"'\", b's', b' the', b' longest', b' word', b' in', b' the', b' English', b' language', b'?'])]\n"
     ]
    }
   ],
   "source": [
    "response = client.models.compute_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the longest word in the English language?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0pb-Kh1xEHU"
   },
   "source": [
    "## Function calling\n",
    "\n",
    "[Function calling](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling) lets you provide a set of tools a model can use to respond to the user's prompt. You create a description of a function in your code, then pass that description to a language model in a request. The response from the model includes the name of a function that matches the description and the arguments to call it with.\n",
    "\n",
    "For more examples of Function Calling, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "2BDQPwgcxRN3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionCall(id=None, args={'destination': 'Paris'}, name='get_destination')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_destination = FunctionDeclaration(\n",
    "    name=\"get_destination\",\n",
    "    description=\"Get the destination that the user wants to go to\",\n",
    "    parameters={\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"destination\": {\n",
    "                \"type\": \"STRING\",\n",
    "                \"description\": \"Destination that the user wants to go to\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "destination_tool = Tool(\n",
    "    function_declarations=[get_destination],\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"I'd like to travel to Paris.\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[destination_tool],\n",
    "        temperature=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "response.candidates[0].content.parts[0].function_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA1Sn-VQE6_J"
   },
   "source": [
    "## Use context caching\n",
    "\n",
    "[Context caching](https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview) lets you to store frequently used input tokens in a dedicated cache and reference those tokens for subsequent requests. This eliminates the need to repeatedly pass the same set of tokens to a model.\n",
    "\n",
    "**Note**: Context caching is only available for stable models with fixed versions (for example, `gemini-2.0-flash-001`). You must include the version postfix (for example, the `-001`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqxTesUPIkNC"
   },
   "source": [
    "#### Create a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "adsuvFDA6xP5"
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are an expert researcher who has years of experience in conducting systematic literature surveys and meta-analyses of different topics.\n",
    "  You pride yourself on incredible accuracy and attention to detail. You always stick to the facts in the sources provided, and never make up new facts.\n",
    "  Now look at the research paper below, and answer the following questions in 1-2 sentences.\n",
    "\"\"\"\n",
    "\n",
    "pdf_parts = [\n",
    "    Part.from_uri(\n",
    "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "    Part.from_uri(\n",
    "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "cached_content = client.caches.create(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    config=CreateCachedContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        contents=pdf_parts,\n",
    "        ttl=\"3600s\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBdQNHEoJmC5"
   },
   "source": [
    "#### Use a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "N8EhgCzlIoFI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The research paper introduces Gemini, a new family of multimodal models that exhibit remarkable capabilities across image, audio, video, and text understanding, developed at Google.\n",
      "The goal of the paper is to present Gemini, a family of highly capable multimodal models, and describe its capabilities in language, coding, reasoning, and multimodal tasks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    contents=\"What is the research goal shared by these research papers?\",\n",
    "    config=GenerateContentConfig(\n",
    "        cached_content=cached_content.name,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azhqrdiCer19"
   },
   "source": [
    "#### Delete a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "rAUYcfOUdeoi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteCachedContentResponse()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.caches.delete(name=cached_content.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43be33d2672b"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch prediction\n",
    "\n",
    "While online (synchronous) responses limits you to one input request at a time, [batch predictions for the Gemini API in Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini) allow you to send a large number of requests to Gemini in a single batch request. Then, the model responses asynchronously populate to your storage output location in [Cloud Storage](https://cloud.google.com/storage/docs/introduction) or [BigQuery](https://cloud.google.com/bigquery/docs/storage_overview).\n",
    "\n",
    "Batch predictions are generally more efficient and cost-effective than online predictions when processing a large number of inputs that are not latency sensitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adf948ae326b"
   },
   "source": [
    "### Prepare batch inputs\n",
    "\n",
    "The input for batch requests specifies the items to send to your model for prediction.\n",
    "\n",
    "Batch requests for Gemini accept BigQuery storage sources and Cloud Storage sources. Learn more about the batch input formats in the [Batch text generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini#prepare_your_inputs) page.\n",
    "\n",
    "This tutorial uses Cloud Storage as an example. The requirements for Cloud Storage input are:\n",
    "\n",
    "- File format: [JSON Lines (JSONL)](https://jsonlines.org/)\n",
    "- Located in `us-central1`\n",
    "- Appropriate read permissions for the service account\n",
    "\n",
    "Each request that you send to a model can include parameters that control how the model generates a response. Learn more about Gemini parameters in the [Experiment with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values) page.\n",
    "\n",
    "This is one of the example requests in the input JSONL file `batch_requests_for_multimodal_input_2.jsonl`:\n",
    "\n",
    "```json\n",
    "{\"request\":{\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/office-desk.jpeg\", \"mime_type\": \"image/jpeg\"}}]}],\"generationConfig\":{\"temperature\": 0.4}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "81b25154a51a"
   },
   "outputs": [],
   "source": [
    "INPUT_DATA = \"gs://cloud-samples-data/generative-ai/batch/batch_requests_for_multimodal_input_2.jsonl\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2031bb3f44c2"
   },
   "source": [
    "### Prepare batch output location\n",
    "\n",
    "When a batch prediction task completes, the output is stored in the location specified in your request.\n",
    "\n",
    "- The location is in the form of a Cloud Storage or BigQuery URI prefix, for example:\n",
    "`gs://path/to/output/data` or `bq://projectId.bqDatasetId`.\n",
    "\n",
    "- If not specified, `gs://STAGING_BUCKET/gen-ai-batch-prediction` is used for Cloud Storage source and `bq://PROJECT_ID.gen_ai_batch_prediction.predictions_TIMESTAMP` is used for BigQuery source.\n",
    "\n",
    "This tutorial uses a Cloud Storage bucket as an example for the output location.\n",
    "\n",
    "- You can specify the URI of your Cloud Storage bucket in `BUCKET_URI`, or\n",
    "- if it is not specified, a new Cloud Storage bucket in the form of `gs://PROJECT_ID-TIMESTAMP` is created for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "fddd98cd84cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://qwiklabs-gcp-04-442e8e076bbd-20250513151141/...\n"
     ]
    }
   ],
   "source": [
    "BUCKET_URI = \"[your-cloud-storage-bucket]\"  # @param {type:\"string\"}\n",
    "\n",
    "if BUCKET_URI == \"[your-cloud-storage-bucket]\":\n",
    "    TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    BUCKET_URI = f\"gs://{PROJECT_ID}-{TIMESTAMP}\"\n",
    "\n",
    "! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7da62c98880"
   },
   "source": [
    "### Send a batch prediction request\n",
    "\n",
    "To make a batch prediction request, you specify a source model ID, an input source, and an output location where Vertex AI stores the batch prediction results.\n",
    "\n",
    "To learn more, see the [Batch prediction API](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/batch-prediction-api) page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "7ed3c2925663"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/285346665666/locations/europe-west1/batchPredictionJobs/5992969559773020160'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_job = client.batches.create(\n",
    "    model=MODEL_ID,\n",
    "    src=INPUT_DATA,\n",
    "    config=CreateBatchJobConfig(dest=BUCKET_URI),\n",
    ")\n",
    "batch_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1bd49ff2c9e"
   },
   "source": [
    "Print out the job status and other properties. You can also check the status in the console at https://console.cloud.google.com/vertex-ai/batch-predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "ee2ec586e4f1"
   },
   "outputs": [],
   "source": [
    "batch_job = client.batches.get(name=batch_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64eaf082ecb0"
   },
   "source": [
    "Optionally, you can list all the batch prediction jobs in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "da8e9d43a89b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/285346665666/locations/europe-west1/batchPredictionJobs/5992969559773020160 2025-05-13 15:11:45.391192+00:00 JobState.JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "for job in client.batches.list():\n",
    "    print(job.name, job.create_time, job.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de178468ba15"
   },
   "source": [
    "### Wait for the batch prediction job to complete\n",
    "\n",
    "Depending on the number of input items that you submitted, a batch generation task can take some time to complete. Use the following code to check the job status and wait for the job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "c2187c091738"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job failed: None\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Refresh the job until complete\n",
    "while batch_job.state == \"JOB_STATE_RUNNING\":\n",
    "    time.sleep(5)\n",
    "    batch_job = client.batches.get(name=batch_job.name)\n",
    "\n",
    "# Check if the job succeeds\n",
    "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
    "    print(\"Job succeeded!\")\n",
    "else:\n",
    "    print(f\"Job failed: {batch_job.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0156eaf66675"
   },
   "source": [
    "### Retrieve batch prediction results\n",
    "\n",
    "When a batch prediction task is complete, the output of the prediction is stored in the location specified in your request. It is also available in `batch_job.dest.bigquery_uri` or `batch_job.dest.gcs_uri`.\n",
    "\n",
    "Example output:\n",
    "\n",
    "```json\n",
    "{\"status\": \"\", \"processed_time\": \"2024-11-13T14:04:28.376+00:00\", \"request\": {\"contents\": [{\"parts\": [{\"file_data\": null, \"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/gardening-tools.jpeg\", \"mime_type\": \"image/jpeg\"}, \"text\": null}], \"role\": \"user\"}], \"generationConfig\": {\"temperature\": 0.4}}, \"response\": {\"candidates\": [{\"avgLogprobs\": -0.10394711927934126, \"content\": {\"parts\": [{\"text\": \"Here's a list of the objects in the image:\\n\\n* **Watering can:** A green plastic watering can with a white rose head.\\n* **Plant:** A small plant (possibly oregano) in a terracotta pot.\\n* **Terracotta pots:** Two terracotta pots, one containing the plant and another empty, stacked on top of each other.\\n* **Gardening gloves:** A pair of striped gardening gloves.\\n* **Gardening tools:** A small trowel and a hand cultivator (hoe).  Both are green with black handles.\"}], \"role\": \"model\"}, \"finishReason\": \"STOP\"}], \"modelVersion\": \"gemini-2.0-flash-001@default\", \"usageMetadata\": {\"candidatesTokenCount\": 110, \"promptTokenCount\": 264, \"totalTokenCount\": 374}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "c2ce0968112c"
   },
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import pandas as pd\n",
    "\n",
    "fs = fsspec.filesystem(\"gcs\")\n",
    "\n",
    "file_paths = fs.glob(f\"{batch_job.dest.gcs_uri}/*/predictions.jsonl\")\n",
    "\n",
    "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
    "    # Load the JSONL file into a DataFrame\n",
    "    df = pd.read_json(f\"gs://{file_paths[0]}\", lines=True)\n",
    "\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f81ccNPjiVzH"
   },
   "source": [
    "## Get text embeddings\n",
    "\n",
    "You can get text embeddings for a snippet of text by using the `embed_content` method. While all models produce an output with 768 dimensions by default, some models give users the option to choose an output dimensionality between `1` and `768`. See [Vertex AI text embeddings API](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "zGOCzT7y31rk"
   },
   "outputs": [],
   "source": [
    "TEXT_EMBEDDING_MODEL_ID = \"text-embedding-005\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "s94DkG5JewHJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ContentEmbedding(values=[-0.07045752555131912, 0.032266948372125626, 0.016506649553775787, -0.03975583240389824, -0.03190124034881592, 0.05326863378286362, 0.04023094102740288, 0.015076016075909138, -0.032303109765052795, 0.006484445184469223, -0.0024321170058101416, 0.011177822947502136, 0.0337241068482399, -0.025560013949871063, -0.018441712483763695, 0.019186269491910934, 0.015283623710274696, -0.03598617762327194, -0.04253664240241051, -0.028812961652874947, 0.0026159759145230055, -0.0029092272743582726, -0.017144568264484406, -0.014058108441531658, -0.06653711199760437, 0.0311944168061018, -0.00982401892542839, 0.011957993730902672, 0.03449905663728714, -0.0309318657964468, 0.030545806512236595, -0.006762423552572727, 0.05796542391180992, 0.06404424458742142, -0.021345125511288643, -0.05859833583235741, 0.015458089299499989, -0.025612354278564453, 0.0018945581978186965, -0.033962976187467575, 0.03931983560323715, -0.06359726935625076, -0.10694780200719833, 0.014972669072449207, 0.036555271595716476, 0.08227287977933884, -0.04199172928929329, 0.01982123777270317, -0.0801253467798233, 0.022590266540646553, 0.039794985204935074, 0.034575168043375015, -0.006564989686012268, -0.02023416757583618, -0.031998686492443085, 0.0423390232026577, -0.02992982417345047, -0.013591731898486614, 0.033661145716905594, 0.07082237303256989, 0.008499796502292156, -0.04865473136305809, -0.002240255009382963, 0.000133246008772403, -0.06205907464027405, -0.09021010249853134, 0.003017326118424535, 0.009957147762179375, 0.036790668964385986, 0.045988310128450394, -0.025111189112067223, -0.022036859765648842, 0.07070532441139221, 0.009957482106983662, -0.005083127412945032, -0.03338221460580826, 0.02343789115548134, 0.01667286455631256, -0.0507337749004364, -0.04134524613618851, -0.05723491683602333, 0.005803215783089399, -0.007348287384957075, -0.04580540210008621, -0.03365001454949379, -0.05468742549419403, -0.004117684438824654, -0.05136139690876007, -0.03516760095953941, -0.047195497900247574, 0.019801415503025055, -0.068564273416996, 0.015214627608656883, 0.02218555472791195, 0.014502515085041523, 0.0535530149936676, 0.07894973456859589, 0.10622807592153549, -0.10025843977928162, 0.003122666385024786, -0.04228673502802849, 0.06404853612184525, -0.0014932050835341215, -0.003603595308959484, -0.02391131781041622, -0.01967342011630535, -0.002356563229113817, -0.020007967948913574, 0.014360041357576847, -0.06779304891824722, -0.016298115253448486, 0.0023577238898724318, -0.005765872076153755, 0.0023203580640256405, -0.013192295096814632, -0.056412022560834885, 0.007363807875663042, -0.05415644124150276, -0.05731481686234474, -0.0473717637360096, -0.010678972117602825, -0.03670541197061539, 0.029316440224647522, 0.07834959030151367, -0.050921354442834854, 0.023325718939304352, 0.01781112514436245, 0.004857619293034077], statistics=ContentEmbeddingStatistics(truncated=False, token_count=18.0)), ContentEmbedding(values=[-0.040600020438432693, 0.01235074084252119, -0.019680218771100044, -0.01208257395774126, -0.023434359580278397, 0.03601039946079254, 0.07041875272989273, 0.016988124698400497, -0.03544733673334122, 0.04181361943483353, 0.0034712895285338163, -0.06320357322692871, -0.011010679416358471, -0.03305432200431824, -0.015262528322637081, 0.009415878914296627, 0.03613364323973656, -0.002960699377581477, -0.004089315887540579, -0.015835143625736237, 0.03513144701719284, -0.01699681580066681, -0.027534503489732742, -0.02328488416969776, -0.0430714376270771, 0.009024102240800858, -0.012769447639584541, 0.011488243006169796, 0.004786378238350153, -0.012272956781089306, 0.014330725185573101, -0.00987081415951252, 0.07926638424396515, 0.06036229431629181, -0.008039862848818302, -0.040001142770051956, 0.02174343727529049, -0.0169086717069149, -0.041982974857091904, -0.011756213381886482, 0.04715228080749512, -0.04051115736365318, -0.058620575815439224, 0.04088404402136803, 0.026120632886886597, 0.054874539375305176, -0.03935108333826065, 0.023588253185153008, -0.08596636354923248, -0.01911880262196064, 0.02538587525486946, 0.05626749247312546, -0.05553530529141426, -0.02841579169034958, -0.06140869855880737, 0.06322570890188217, 0.02559659630060196, 0.02311590500175953, 0.003995103761553764, 0.04910663887858391, 0.010585177689790726, -0.018742388114333153, -0.005701154004782438, 0.023866448551416397, -0.017191538587212563, -0.02421025186777115, -0.014561029151082039, 0.025130953639745712, 0.0678071528673172, 0.03193819150328636, -0.029276639223098755, -0.032992564141750336, 0.031784553080797195, 0.014396755956113338, -0.04835090413689613, -0.05750396102666855, 0.056417543441057205, 0.026612402871251106, 0.003706221003085375, -0.006792127620428801, -0.06691009551286697, 0.03880560025572777, -0.011049091815948486, -0.031246468424797058, -0.030212681740522385, -0.05478524789214134, -0.0013406776124611497, -0.05040643364191055, -0.024219637736678123, -0.05464452505111694, 0.01680794544517994, -0.1249748095870018, 0.059532176703214645, 0.004978098440915346, 0.017065662890672684, 0.046341974288225174, 0.051061924546957016, 0.08931786566972733, -0.0840212032198906, -0.01699705980718136, -0.032077014446258545, 0.015147105790674686, 0.024135185405611992, -0.0014089193427935243, -0.017831386998295784, -0.018690910190343857, -0.056389641016721725, -0.06188696622848511, 0.0756433978676796, -0.01442782673984766, -0.0014380979118868709, 0.0031280883122235537, 0.003938968759030104, -0.0068269590847194195, -0.01963425800204277, -0.05851637199521065, -0.00042248715180903673, -0.05001836270093918, -0.04916193708777428, -0.06605798751115799, 0.008464677259325981, -0.0366244800388813, 0.02593417279422283, 0.05368606746196747, -0.005153220146894455, 0.014385443180799484, 0.050983212888240814, 0.01596856489777565], statistics=ContentEmbeddingStatistics(truncated=False, token_count=10.0)), ContentEmbedding(values=[-0.08152230829000473, 0.013152849860489368, -0.03257665038108826, 0.03197991102933884, -0.04253409057855606, 0.06282426416873932, 0.058349333703517914, -0.021165911108255386, -0.006778216455131769, 0.033225346356630325, 0.010542809031903744, -0.07519274204969406, 0.01183386892080307, -0.018258651718497276, -0.03174315020442009, -0.017948169261217117, 0.04329204186797142, -0.04167334362864494, 0.008950967341661453, 0.015369968488812447, 0.019139297306537628, -0.009914790280163288, -0.017411252483725548, -0.009244228713214397, -0.03578120842576027, -0.013013222254812717, -0.02414533868432045, 0.001035756547935307, -0.016214260831475258, -0.027162395417690277, -0.0054807160049676895, -0.05758097395300865, 0.06951066851615906, 0.02333156019449234, -0.011622974649071693, -0.05659351125359535, 0.03599949926137924, -0.0431552454829216, -0.007645015139132738, 0.0025956102181226015, 0.02833838388323784, -0.08793970197439194, -0.07204460352659225, 0.011749079450964928, 0.045021604746580124, 0.05936109647154808, -0.021462401375174522, -0.0005923219723626971, -0.07067076116800308, -0.0386657677590847, 0.0003290101885795593, 0.0902971550822258, -0.028865663334727287, -0.07067389041185379, -0.06398309022188187, 0.053239382803440094, 0.014394483529031277, 0.028766246512532234, 0.02622423693537712, 0.08315692096948624, 0.012001410126686096, -0.01796274073421955, -0.04735063016414642, 0.04483312740921974, 0.016355058178305626, -0.04842698946595192, 0.005748494993895292, 0.03360263258218765, 0.06504500657320023, 0.010348251089453697, -0.006146855186671019, -0.051634009927511215, 0.043523456901311874, -0.0038610969204455614, -0.03951967507600784, -0.025342371314764023, 0.07996957004070282, 0.02915206179022789, 0.011532247997820377, -0.010988202877342701, -0.03222794085741043, 0.01302323117852211, -0.02658616378903389, -0.013707761652767658, -0.05092770233750343, -0.013486696407198906, -0.04149186611175537, -0.02143094129860401, -0.03536752238869667, -0.0272765401750803, 0.014350875280797482, -0.08868151903152466, 0.0291373822838068, -0.005801372230052948, -0.01348965521901846, 0.021268710494041443, 0.063230000436306, 0.08822762966156006, -0.10215342044830322, -0.013053481467068195, -0.03695044666528702, -0.02952970378100872, 0.015198579989373684, -0.015535973012447357, -0.041377533227205276, -0.014440011233091354, -0.05201679840683937, -0.030554667115211487, 0.07688234746456146, -0.01912546344101429, 0.017124148085713387, -0.02142583578824997, 0.021098323166370392, 0.003955203574150801, -0.05848342180252075, -0.03201206028461456, -0.024658098816871643, -0.05020337179303169, -0.04261450842022896, -0.03640974313020706, 0.01540687121450901, -0.03000785782933235, 0.03421954810619354, 0.019490478560328484, 0.014071287587285042, 0.056027792394161224, 0.051268234848976135, 0.01663997955620289], statistics=ContentEmbeddingStatistics(truncated=False, token_count=13.0))]\n"
     ]
    }
   ],
   "source": [
    "response = client.models.embed_content(\n",
    "    model=TEXT_EMBEDDING_MODEL_ID,\n",
    "    contents=[\n",
    "        \"How do I get a driver's license/learner's permit?\",\n",
    "        \"How do I renew my driver's license?\",\n",
    "        \"How do I change my address on my driver's license?\",\n",
    "    ],\n",
    "    config=EmbedContentConfig(output_dimensionality=128),\n",
    ")\n",
    "\n",
    "print(response.embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQwiONFdVHw5"
   },
   "source": [
    "# What's next\n",
    "\n",
    "- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai).\n",
    "- Explore AI models in [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "intro_genai_sdk.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
